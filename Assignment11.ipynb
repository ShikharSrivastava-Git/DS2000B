{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 11: Model Selection, Regularized regression and final practice\n",
    "## Learning goals \n",
    "This homework includes some repeated task and variations on model fitting, and model comparision, which should prepare you optimally for the final. \n",
    "The homework also introduces z-standardization and regularized (L2) regression. \n",
    "Try to solve the task in the homework independently, and prepare a cheat-sheet and a file with useful functions, such that you can complete this homework in 3 hrs or less. \n",
    "\n",
    "## Data set \n",
    "The file kaiser.csv contains a subset of data from the Child Health and Development Studies, which investigate a range of topics. One study considered all pregnancies between 1960 and 1967 among women in the Kaiser Foundation Health Plan in the San Francisco East Bay area. Here, we look at the predictor of birth weight of babies, measured in pounds, as well as the occurence of complications in the first 3 month. \n",
    "\n",
    "The data frame stored in babies.csv contains the variables: \n",
    "- gestation:    Gestation period (length of pregnancy) [days]\n",
    "- parity:       1: child the first born 0: Child has older siblings \n",
    "- age:          Age of the mom at time of birth\n",
    "- height:       Height of the baby [cm].\n",
    "- weight:       Weight of the baby [pounds].  \n",
    "- smoke:        Is the mom a smoker / non-smoker? \n",
    "- hospital:     Which hospital was the birth at? Oakland, SanFrancisco, WalnutCreek, SanJose, and Richmond.\n",
    "- complication: Was there a complication within the first 3 month of pregnancy (0: No 1:Yes) \n",
    "\n",
    "## Preliminaries\n",
    "Set up the environment by importing pandas, seaborn, numpy, scipy.optimize and matplotlib. \n",
    "Then add your multiple regression functions (multRegPredict,multRegLossRSS, multRegFit) from the last homeworks. \n",
    "\n",
    "To make it easier - we have done these things already! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.optimize as so\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multRegPredict(b,D,xname):\n",
    "    \"\"\"Prediction function for multipel regression \n",
    "\n",
    "    Args:\n",
    "        b (nd.array): Array of regression coefficients - first is intercept \n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        xname (list): List of strings with names of explanatory variables  \n",
    "\n",
    "    Returns:\n",
    "        yp (nd.array): Predicted y - values \n",
    "    \"\"\"\n",
    "    yp=np.ones(len(D.index))*b[0]        # Intercept \n",
    "    for i in range(len(xname)):          \n",
    "        yp=yp+D[xname[i]]*b[i+1]         # Add each regression value \n",
    "    return yp \n",
    "\n",
    "def multRegLossRSS(b,D,y,xname):\n",
    "    \"\"\"Loss function for OLS multiple regression \n",
    "\n",
    "    Args:\n",
    "        b (nd.array): Array of regression coefficients - first is intercept \n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable \n",
    "        xname (list): List of strings with names of explanatory variables  \n",
    "\n",
    "    Returns:\n",
    "        rss: Current loss\n",
    "        grad: gradient of loss function in respect to parameters  \n",
    "    \"\"\"\n",
    "    predY = multRegPredict(b,D,xname)\n",
    "    res = y-predY\n",
    "    rss = sum(res**2)\n",
    "    grad=np.zeros(len(b))\n",
    "    grad[0]=-2*np.sum(res)\n",
    "    for i in range(len(xname)):\n",
    "        grad[i+1]=-2*np.sum(D[xname[i]]*res)\n",
    "    return (rss,grad)\n",
    "\n",
    "def multRegFit(D,y,xname=[],figure=0,b0=[]):\n",
    "    \"\"\"Fits a multiple regression loss function \n",
    "\n",
    "    Args:\n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable \n",
    "        xname (list): List of strings with names of explanatory variables  \n",
    "        figure (int): Plot figure? Defaults to 0.\n",
    "        b0 (np.ndarray). Initial guess for the parameter vector\n",
    "\n",
    "    Returns:\n",
    "        R2: Fitted R2 value \n",
    "        b: Fitted \n",
    "    \"\"\"\n",
    "    k=len(xname)+1\n",
    "    if (len(b0)!=k):\n",
    "        b0=np.zeros((k,))\n",
    "    RES = so.minimize(multRegLossRSS,b0,args=(D,y,xname),jac=True)\n",
    "    b=RES.x # Results\n",
    "    res = y-np.mean(y)\n",
    "    TSS = sum(res**2)\n",
    "    RSS,deriv = multRegLossRSS(b,D,y,xname)\n",
    "    R2 = 1-RSS/TSS \n",
    "    if (k==2 and figure==1):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.scatter(D[xname[0]],y)\n",
    "        xRange=[min(D[xname[0]]),max(D[xname[0]])]\n",
    "        xp=np.arange(xRange[0],xRange[1],(xRange[1]-xRange[0])/50)\n",
    "        yp=b[0]+b[1]*xp\n",
    "        ax.plot(xp,yp,'r-')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    return (R2,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Multiple regression with discrete variables ( / 30 pts)\n",
    "### Question 1.1 ( / 10pt)\n",
    "Create a dummy variable for Smoker / Non-smoker. Set the value for “Smoker” to 1 and for “Non-smoker” to 0. Estimate a regression model with the dummy variable as a regressor *and birth weight as the response variable.* \n",
    "\n",
    "Report the value of the intercept and slope. What does the intercept and slope value indicate? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 ( / 8pt)\n",
    "Make a boxplot of hospital on the x-axis and birthweight on the y-axis (see HW2 for an example). Which hospital has the lowest overall birth weight?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3  ( / 12pt)\n",
    "Create a set of 4 dummy variables that together code the hospital. Set Walnut Creek to be your comparison group. Run a multiple regression model with the 4 dummy variables as explanatory variables. Report the interecept and slope values. What do the intercept and slope values mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model selection for multiple regression  ( / 35 pts)\n",
    "### Question 2.1 ( / 20 pts)\n",
    "Write a version of the crossvalidation function that does K-fold crossvalidation and works specifically with multRegFit as the fitting function. \n",
    "\n",
    "KfoldCVmultReg(D,y,xname,K=20,fitfcn=multRegFit,param={},predictfcn=multRegPredict):\n",
    "- D: Data Frame with explanatory variables  \n",
    "- y: response variable \n",
    "- xname: List of explanatory variables\n",
    "- K: Number of crossvalidation folds\n",
    "\n",
    "For dividing the data up in K pieces, you can use the following trick to assign a partition index to each of the data-points:\n",
    "```\n",
    "#N = number of data points \n",
    "#K = number of test sets (folds)\n",
    "ind = np.arange(N)\n",
    "ind = np.floor(ind/N*K)\n",
    "```\n",
    "\n",
    "\n",
    "The code should compute and return the crossvalidated R2 and the fitted R2. \n",
    "It should use the entries in the Dictionary to pass them to the function using `fitfcn(D,y,xname,**param)`\n",
    "Run 20-fold crossvalidation on the multiple regression model, with birth weight as the response variable and \n",
    "\n",
    "- age of the mother \n",
    "- smoker (dummy coded) \n",
    "- birth occurred in Oakland? \n",
    "- gestation \n",
    "\n",
    "as explanatory variables. \n",
    "Report R2cv and R2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 ( / 15 pts)\n",
    "Using the R2cv from the 20-fold crossvalidation, determine the best predictive model for birthweight using the following candidate variables \n",
    "\n",
    "- age of mom\n",
    "- smoker (dummy coded) \n",
    "- gestation \n",
    "- parity \n",
    "\n",
    "Start with the R2cv for the full model (Question 2.1)and use backwards step-wise regression to find the best model (the model that increases R2cv the most). Show all steps of your selection procedure. Report the formula of your best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement regularilzed regression to build a better predictive model (/35pts)\n",
    "In this task you will implement regularized regression to try to build a better predictive model for the birthweight of data. \n",
    "Like in Task 2, we will consider the following explanatory variables:\n",
    "\n",
    "- age of the mother \n",
    "- smoker (dummy coded) \n",
    "- birth occurred in Oakland? \n",
    "- gestation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1: Z-standardize the regressors (/8pts)\n",
    "Write a function `zstandardize`, which takes as an input a pandas series or ndarray \n",
    "and returns a z-standardized version of the data \n",
    "\n",
    "Use the function to z-standardize the columns age,gestation,parity, and smokeDummy. \n",
    "\n",
    "Create new columns in the data frame called ageZ,gestationZ,parityZ, and smokeDummyZ.\n",
    "\n",
    "Check that the mean of the new variables in very close to and the std very close to 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2 Implement Ridge regression (L2 regularized regression) (/17pts)\n",
    "\n",
    "To implement ridge regression you need to modify two functions, the most important being the loss function. \n",
    "Make a copy of the function `multRegLossRSS` from assigment 10. \n",
    "Rename it to `ridgeLoss`. Give the function an additional input parameter, namely alpha. Give this a default value of 1.0. \n",
    "\n",
    "Change the loss and the gradient to take into account the regularization. \n",
    "\n",
    "**Note that we are not regularizing the intercept regressor (b0)**\n",
    "\n",
    "Overall the function should take the following input arguments:\n",
    "\n",
    "    Args:\n",
    "        b (nd.array): Array of regression coefficients - first is intercept \n",
    "        D (pd.DataFrame): Pandas data frame with explanatory variables\n",
    "        y (ndarray): Dependent variable \n",
    "        xname (list): List of strings with names of explanatory variables\n",
    "        alpha (float): Regularization parameter \n",
    "\n",
    "    Returns:\n",
    "        loss: Current loss\n",
    "        grad: gradient of loss function in respect to parameters  \n",
    "\n",
    "Then make a copy of `multRegFit` from the last homework and rename it to `ridgeFit`. \n",
    "Again, you need to add an additional input parameter (alpha) to the function. \n",
    "Alpha needs to be passed to your loss function (`ridgeLoss`) when you call so.minimize: \n",
    "\n",
    "`so.minimize(ridgeLoss,b0,args=(D,y,xname,alpha),jac=True)`\n",
    "\n",
    "You need to take care when you calculate R2 of the fit - Since ridgeLoss does not return the \n",
    "residual-sum-of-squares, you need to use the appropriate function to calculate the RSS.\n",
    "\n",
    "To test your function:\n",
    "* Use it to fit a model that explains weight with the explanatory variables `['ageZ','smokeDummyZ','gestationZ','parityZ']`. Note: use the zstandardized version of the variables.  \n",
    "* Do the fit setting `alpha=0` and `alpha=8`, and report both R2 and the regression coefficients (b)\n",
    "* Compare the R2 between the two settings of alpha. Also compare to the one found for normal multiple regression (Question 2.1). What do you see and why?\n",
    "* Compare the regression weights (b) between the two settings of alpha. which regression weights changed and why? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3: Crossvalidate Ridge regression (10pts)\n",
    "Copy your function `KfoldCVmultReg` from Question 2.1, rename it to `KfoldCVridge`, and modify it to work with Ridge regression. \n",
    "That means it needs to take an additional input parameter `alpha` that it passes on to the fitting function. \n",
    "\n",
    "To calculate the R2 and R2cv for the model of `weight` using the explanatory variables `['ageZ','smokeDummyZ','gestationZ','parityZ']`. Like in question 3.2, use the standardized versions of the variables and try both the setting `alpha=0` and `alpha=8`. \n",
    "\n",
    "How to the R2 and R2cv values compare between the two settings of alpha? Do you get better predictive performance than the reduced model that you found using feature selection (Question 2.2)?\n",
    "\n",
    "\n",
    "*Note: If you want, play a bit with the regularization parameter to see if you can find a better setting. What happens when you make `alpha` very large (i.e. 1000)? (this is optional, but educational)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Use logistic regression to predict complications \n",
    "In this task you will create and test a logistic regression model that predicts the presence of a complication in the first three month (0: no complication, 1: complication). \n",
    "\n",
    "** Task 4 of the Homework does not have to be handed in and will not be graded! It is only added here to provide additional preparation and practice for you for the final. So if you are short on time, leave these questions open and solve them when you practice for the final.**\n",
    "\n",
    "### Question 4.1: Improving your logistic regression model code\n",
    "Improve your code for logisitic regression in two ways: \n",
    "\n",
    "1. prevent log(0) errors by making sure that your predicted value never is smaller than 1e-20 or larger than 1-1e-20. (tip you can use the numpy function `clip`)\n",
    "\n",
    "2. Let logisticRegFit take an additional input parameter, telling it whether it should plot a figure or not (figure=1) \n",
    "\n",
    "3. Let logisticRegFit take an additional input parameter, specifying the starting value for the parameters (b0=[]). If b0 is empty, the function should start with a vector off all zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Crossvalidation of logistic models\n",
    "Modify the KfoldCVmultReg function to make it work for logistic regression. As before, use K-fold crossvalidation. The main changes are that \n",
    "- you need to use logisticRegFit and logisticRegPredict as fitfcn and predictfcn respectively. \n",
    "- instead of the crossvalidated R2, your function should return the crossvalidated log-likelihood and non-crossvalidated log-likelihood. \n",
    "\n",
    "Using your function, calculate the the difference in crossvalidated log-likelihood for the model that predicts complications with an intercept only (b0) and a model that predicts complications with an intercept and smokeDummy. \n",
    "From the difference, report the Bayes-Factor between the two models. What do you conclude? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3:\n",
    "Compare the model that uses only smoking (and intercept) as explanatory variable to one that uses additionally (to intercept and smoking) the weight of the baby at birth, age of the mom, or both. Calculate crossvalidated Log-likelihood, which one is the best model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.4 : \n",
    "In the model (['smokeDummy','weight']), how do each of the explanatory variables contribute to the chance of complication? That is, for each variable, would an increase an the variable lead to an increased or decreased probability of a complication? In terms of risk of complications, how many pounds of extra weight can make up for having a smoking mother?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
